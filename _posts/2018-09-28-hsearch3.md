---
title: Hyperparameter Search on a Cluster (Part 3)
layout: post
author: Stanislav Bohm
---

The goal of this blog post is to demonstrate how to use
[Rain](https://github.com/substantic/rain) for a **distributed** hyperparameter
search on a cloud infrastructure.

In previous parts, we used Python MNIST example wrapped into function decorated
by Rain's remote decorator. In this part, we are going to execute external
program that do training for us. Actually, it will again be TensorFlow program,
but now we use it as a block-box with command line interface.

So let us have program ``mnist.py`` that trains a model on MNIST example.
It has the following command line interface:

```
python3 mnist.py --data <PATH_TO_DATA> --dropout <DROPOUT> --size <SIZE>
```

Dropout and size has the same meaning as in the previous examples. Argument
``--data`` expects a path to MNIST data. The program prints accuracy on the
standard output. The full source code of ``mnist.py`` is at the end of this post.


## Executing external programs

Before we start, let us say few words about executing external programs. It is
implemented as ``tasks.Execute``. When this task is executed in a governor. It
creates a local temporary directory and maps input as files. After that, the
specified program is executed. When it is finished, files marked as outputs are
moved out and temporary directory is removed.

The executed program may do whatever it wants in its directory. It should not
use files outside of it since it cannot assume where it is actually executed. If
it needs some files/directories they should be mapped as inputs.
The Rain distribute the data across the cluster as necessary.

![Rain execute]({{ "/assets/images/hsearch/execute.svg" | absolute_url }})


## Initilization

Let start with our Rain program. The full source code is at the end of the blog
post. Initialization is almost the same as in the previous example, except we define
variable ``MNIST_PROGRAM`` which should contain a path to ``mnist.py``. Do not forget
to modify it to your actual path.

```python
MNIST_PROGRAM = "/path/to/mnist.py"  # <-- The only difference

SIZES = [32, 64, 128, 256]
DROPOUTS = [0.0, 0.2, 0.4, 0.6, 0.8]

client = Client("localhost", 7210)
session = client.new_session("MNIST test", default=True)
```

## Data download

Now we need to download the data. We want to do it only once, hence we decomple
this step from training. In previous version, we haved used Tensorflow build-in
function to download and we have distributed pickled data. Now we directly
download data as a file through external program ``wget``. It takes an URL and
(in our case) creates file ``mnist.npz``.

```python
mnist_data = tasks.Execute(
    "wget https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz",
    output_paths=["mnist.npz"])
```

Argument ``output_paths`` serves to define what files should be taken as output
of the task. It is a relative path to the working directory of the task.

## Training

Creating training tasks is very similar to previous version, insted of calling
``train_mnist`` we call our ``mnist.py``.

```python
mnist_tasks = [
    tasks.Execute(
        ["python3", MNIST_PROGRAM, "--data", mnist_data, "--size", str(size), "--dropout", str(dropout)],
        stdout=True,
        name="mnist size={} dropout={}".format(size, dropout))
    for size, dropout in itertools.product(SIZES, DROPOUTS)
]

for task in mnist_tasks:
    task.output.keep()

```

Note, that we have directly passed our downloading task as the 4th element of the
list defining the arguments. In such case, the data object produced by
``mnist_data`` is mapped to randomly named file into working directory of the
execute task. This random name is put into arguments for ``mnist.py`` on the
place where ``mnist_data`` are placed. In other words we are execting command
like this:

```
python3 /path/to/mnist.py --data <RANDOM_NAME_WHERE_DATA_ARE_MAPPED> --size <SIZE> -- data <DROPOUT>
```

The ``stdout`` argument defines that we want to capture a standard
output and use it as the result of the task.


## Getting the result

Getting results is again almost the same as in the previous case. The only
difference is that we do not get pickled objects but raw data captured on
standard outputs of executed programs. Hence we have to covert them into floats:

```python
session.submit()

accuracies = [float(task.output.fetch().get_str()) for task in mnist_tasks]
```

This is all. Now you can test the pipeline locally in the same way as previous
one, e.g.:

```
rain start --simple
python3 <our-rain-script.py>
```


## Running on Exoscale

For running on Exoscale we just need only to copy ``mnist.py`` on all nodes:

```
python3 exoscale.py scp /path/to/mnist.py mnist.py
```

This copies the script to all nodes into their home directories (that is
``/home/ubuntu``). The last thing that we need to modify is variable
``MNIST_PROGRAM``:

```
MNIST_PROGRAM = "/home/ubuntu/mnist.py
```

Now, the script is ready to be executed on Exoscale cluster.

## Full source code of the example

```python
from rain.client import Client, tasks
import tensorflow as tf

import itertools
import numpy as np
import matplotlib.pyplot as plt

MNIST_PROGRAM = "/path/to/mnist.py"

SIZES = [32, 64, 128, 256]
DROPOUTS = [0.0, 0.2, 0.4, 0.6, 0.8]

client = Client("localhost", 7210)
session = client.new_session("MNIST test", default=True)

mnist_data = tasks.Execute(
    "wget https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz",
    output_paths=["mnist.npz"])

mnist_tasks = [
    tasks.Execute(
        ["python3", MNIST_PROGRAM, "--data", mnist_data, "--size", str(size), "--dropout", str(dropout)],
        stdout=True,
        name="mnist size={} dropout={}".format(size, dropout))
    for size, dropout in itertools.product(SIZES, DROPOUTS)
]

for task in mnist_tasks:
    task.output.keep()

session.submit()

accuracies = [float(task.output.fetch().get_str()) for task in mnist_tasks]


def make_heatmap(ax, data):
    data = np.array(data).reshape((len(SIZES), len(DROPOUTS)))

    im = ax.imshow(data)
    ax.set_yticklabels(SIZES)
    ax.set_xticklabels(DROPOUTS)
    ax.set_yticks(np.arange(len(SIZES)))
    ax.set_xticks(np.arange(len(DROPOUTS)))

    for j in range(len(DROPOUTS)):
        for i in range(len(SIZES)):
            text = ax.text(j, i, data[i, j],
                           ha="center", va="center", color="w")


fig, ax1 = plt.subplots()

make_heatmap(ax1, accuracies)

fig.tight_layout()
plt.show()
```

## File 'mnist.py'

```python
import tensorflow as tf
import argparse
import numpy as np


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', help='Path to data', default=None)
    parser.add_argument('--size', type=int, help='Size of hidden layer', default=256)
    parser.add_argument('--dropout', type=float, help='Dropout', default=0.2)
    args = parser.parse_args()

    with np.load(args.data) as f:
        x_train, y_train = f['x_train'], f['y_train']
        x_test, y_test = f['x_test'], f['y_test']

    x_train, x_test = x_train / 255.0, x_test / 255.0

    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(args.size, activation=tf.nn.relu),
        tf.keras.layers.Dropout(args.dropout),
        tf.keras.layers.Dense(10, activation=tf.nn.softmax)
    ])
    model.compile(optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])

    model.fit(x_train, y_train, epochs=1, verbose=False)
    result = model.evaluate(x_test, y_test, verbose=False)
    print(result[1])


if __name__ == "__main__":
    main()
```

